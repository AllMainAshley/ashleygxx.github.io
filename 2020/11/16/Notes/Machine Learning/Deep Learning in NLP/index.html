<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="ç¨‹æµ·ç›"><meta name="copyright" content="ç¨‹æµ·ç›"><meta name="generator" content="Hexo 5.2.0"><meta name="theme" content="hexo-theme-yun"><title>Deep Learning in NLP | Ashley</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/star-markdown-css@0.1.24/dist/yun/yun-markdown.min.css"><script src="//at.alicdn.com/t/font_1140697_j5gk85dg4pf.js" async></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script>document.addEventListener("DOMContentLoaded", () => {
  Yun.utils.renderKatex();
});</script><script src="https://cdn.jsdelivr.net/npm/pjax@latest/pjax.min.js" defer></script><script src="/js/pjax.js" defer></script><script src="https://cdn.jsdelivr.net/npm/vue@2.6.11"></script><link rel="icon" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="alternate icon" href="/yun.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><script id="yun-config">
    const Yun = window.Yun || {};
    window.CONFIG = {"hostname":"allmainashley.github.io","root":"/","title":"ç›å§œè‘±èŠ±é±¼","version":"1.6.1","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}.","hits":"${hits} results found","hits_time":"${hits} results found in ${time} ms"},"anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"local_search":{"path":"/search.xml"},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/utils.js"></script><script src="/js/hexo-theme-yun.js"></script><link rel="alternate" href="/atom.xml" title="Ashley" type="application/atom+xml"><meta name="description" content="Reading notes ofã€ŠDeep Learning in NLPã€‹âœ¨ It is almost about natural language processing functions explainations.">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning in NLP">
<meta property="og:url" content="https://allmainashley.github.io/2020/11/16/Notes/Machine%20Learning/Deep%20Learning%20in%20NLP/index.html">
<meta property="og:site_name" content="Ashley">
<meta property="og:description" content="Reading notes ofã€ŠDeep Learning in NLPã€‹âœ¨ It is almost about natural language processing functions explainations.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/AllMainAshley/CDN@master/images/20201116213953.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/AllMainAshley/CDN@master/images/20201116214144.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/AllMainAshley/CDN@master/images/20201116214304.jpg">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/AllMainAshley/CDN@master/images/20201116214234.png">
<meta property="article:published_time" content="2020-11-16T07:14:34.429Z">
<meta property="article:modified_time" content="2021-01-15T15:42:02.139Z">
<meta property="article:author" content="ç¨‹æµ·ç›">
<meta property="article:tag" content="Tensorflow">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/AllMainAshley/CDN@master/images/20201116213953.png"><script src="/js/ui/mode.js"></script></head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="Table of Contents"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="Overview"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="ç¨‹æµ·ç›"><img width="96" loading="lazy" src="/images/1.jpg" alt="ç¨‹æµ·ç›"><span class="site-author-status" title="Fall in love with lsh.">ğŸ’˜</span></a><div class="site-author-name"><a href="/about/">ç¨‹æµ·ç›</a></div><a class="site-name" href="/about/site.html">Ashley</a><sub class="site-subtitle">Face to new life and SAY HEY.</sub><div class="site-desciption">åšä¸ªå¿«ä¹çš„ç¬¨è›‹</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="Home"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="/archives/" title="Archives"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">22</span></a></div><div class="site-state-item"><a href="/categories/" title="Categories"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">11</span></a></div><div class="site-state-item"><a href="/tags/" title="Tags"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">23</span></a></div><a class="site-state-item hty-icon-button" href="/about/#comment" title="ç•™è¨€æ¿"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-clipboard-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/AllMainAshley" title="GitHub" target="_blank" style="color:#6e5494"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:ashleyallmain@qq.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="æˆ‘çš„å°ä¼™ä¼´ä»¬" style="color:dodgerblue"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-genderless-line"></use></svg></a><a class="links-item hty-icon-button" href="/girls/" title="å–œæ¬¢çš„å¥³å­©å­" style="color:hotpink"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-women-line"></use></svg></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-contrast-2-line"></use></svg></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Data-Preparation"><span class="toc-number">1.</span> <span class="toc-text">Data Preparation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Summary"><span class="toc-number">1.1.</span> <span class="toc-text">Summary</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Additional-Text-Cleaning-Considerations"><span class="toc-number">1.2.</span> <span class="toc-text">Additional Text Cleaning Considerations</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Bag-of-Words-Model%EF%BC%88Bow%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">Bag-of-Words Modelï¼ˆBowï¼‰</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">2.1.</span> <span class="toc-text">ä¼˜ç¼ºç‚¹</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Prepare-Data-With-Keras"><span class="toc-number">3.</span> <span class="toc-text">Prepare Data With Keras</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Bags-of-Words"><span class="toc-number">4.</span> <span class="toc-text">Bags of Words</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article"><link itemprop="mainEntityOfPage" href="https://allmainashley.github.io/2020/11/16/Notes/Machine%20Learning/Deep%20Learning%20in%20NLP/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="ç¨‹æµ·ç›"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="Ashley"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Deep Learning in NLP</h1><div class="post-meta"><div class="post-time" style="display:block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="Created: 2020-11-16 15:14:34" itemprop="dateCreated datePublished" datetime="2020-11-16T15:14:34+08:00">2020-11-16</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-2-line"></use></svg></span> <time title="Modified: 2021-01-15 23:42:02" itemprop="dateModified" datetime="2021-01-15T23:42:02+08:00">2021-01-15</time></div><span class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="Word count in article"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="Word count in article">3.9k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="Reading time"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="Reading time">17m</span></span></span><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/Notes/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">Notes</span></a></span> > <span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/Notes/Machine-Learning/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">Machine Learning</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/Tensorflow/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">Tensorflow</span></a><a class="tag-item" href="/tags/NLP/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">NLP</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body" style="--smc-primary:#0078E7;"><p> Reading notes ofã€ŠDeep Learning in NLPã€‹âœ¨</p>
<p>It is almost about natural language processing functions explainations.</p>
<a id="more"></a>

<h1 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h1><ul>
<li><p><strong>Split by Whitespace</strong></p>
<p>æ–‡ç« è¢«ç©ºæ ¼åˆ†å¼€ï¼›</p>
<p>æ ‡ç‚¹ç¬¦å·å’Œå‰ä¸€ä¸ªå•è¯è¿åœ¨ä¸€èµ·ï¼ˆæ¯”å¦‚ <code>&#39;dream.&#39;</code>ï¼‰ï¼›</p>
<p>**<font color=indianre>ä¼˜ç‚¹ï¼š</font>**éƒ¨åˆ†æ ‡ç‚¹ç¬¦å·è¢«ä¿ç•™ï¼ˆæ¯”å¦‚<code>&quot;wasn&#39;t&quot;</code>å’Œ<code>&#39;armour-like&#39;</code>ï¼‰</p>
<p>**<font color=tomato>ç¼ºç‚¹:</font>**ç¼©å†™è¯è¢«åˆ†å¼€ï¼ˆæ¯”å¦‚<code>What&#39;s </code>å˜ä¸º<code>&#39;&quot;What\&#39;s&#39;</code>ï¼‰ï¼›</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">text=file.read()</span><br><span class="line">words=text.split()</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Select Words</strong></p>
<p>æ–‡ç« ä»¥å•è¯å½¢å¼è¢«åˆ†å¼€ï¼›</p>
<p>å•è¯å»é™¤äº†æ ‡ç‚¹ç¬¦å·ï¼›</p>
<p>ç¼ºç‚¹ï¼š<code>armour-like</code>å˜æˆä¸¤ä¸ªè¯ <code>&#39;armour&#39;,&#39;like&#39; </code>ã€<code>What&#39;s</code>å˜æˆ<code>&#39;What&#39;,&#39;s&#39;</code>ï¼›</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">words=re.split(<span class="string">r&#x27;\W+&#x27;</span>,text)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Split by Whitespace and Remove Punctuation</strong></p>
<p>æ–‡ç« è¢«ç©ºæ ¼åˆ†å¼€ï¼Œå¹¶å»é™¤æ ‡ç‚¹ç¬¦å·ã€‚éœ€è¦ç”¨åˆ°<code>string.punctuation</code>,æ‰“å°çš„ç»“æœæ˜¯ <font color=LighCoral>*<em>!â€#$%&amp;â€™()</em>+,-./:;&lt;=&gt;?@[]^_`{|}~**</font></p>
<p><code>What&#39;s</code>å˜ä¸º<code>&#39;Whats&#39;</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">words = text.split()</span><br><span class="line">re_punc=re.compile(<span class="string">&#x27;[%s]&#x27;</span>%re.escape(string.punctuation))</span><br><span class="line">stripped=[re_punc.sub(<span class="string">&#x27;&#x27;</span>,w) <span class="keyword">for</span> w <span class="keyword">in</span> words]</span><br></pre></td></tr></table></figure>

<p>æœ‰äº›å­—ç¬¦æ— æ³•è¢«æ‰“å°ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨åŒæ ·çš„æ–¹æ³•ç­›å»è¿™äº›å­—ç¬¦ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">words = text.split()</span><br><span class="line">re_print=re.compile(<span class="string">&#x27;[%s]&#x27;</span>%re.escape(string.printable))</span><br><span class="line">stripped=[re_print.sub(<span class="string">&#x27;&#x27;</span>,w) <span class="keyword">for</span> w <span class="keyword">in</span> words]</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Normalizing Case</strong></p>
<p>æŠŠæ‰€æœ‰è¯è½¬æ¢ä¸ºç›¸åŒå½¢å¼ï¼ˆå°å†™ï¼‰</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">words = text.split()</span><br><span class="line">words = [word.lower() <span class="keyword">for</span> word <span class="keyword">in</span> words]</span><br><span class="line">print(words[:<span class="number">100</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>sent_tokenize</strong>ï¼ˆä½¿ç”¨nltkåº“ï¼‰</p>
<p>æ–‡ç« ä»¥å¥å­çš„å½¢å¼åˆ’åˆ†ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> sent_tokenize</span><br><span class="line">sentences = sent_tokenize(text)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>word_tokenize</strong></p>
<p>æ–‡ç« ä»¥è¯çš„å½¢å¼åˆ’åˆ†ï¼ˆæœ‰äº›æ ‡ç‚¹ç¬¦å·ä¹Ÿè¢«åˆ’åˆ†æˆç«‹tokenï¼Œä½†æˆ‘ä»¬å¯ä»¥æ»¤é™¤å®ƒä»¬ï¼‰ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> word_tokenize</span><br><span class="line">tokens = word_tokenize(text)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Filter Out Punctuation</strong><br>å¯ä»¥é€šè¿‡éå†tokenâ€™ï¼Œå¹¶åªä¿ç•™é‚£äº›æ˜¯å­—æ¯çš„tokenï¼ˆpythonå†…ç½®å‡½æ•°<code>isalpha()</code>ï¼‰ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokens = word_tokenize(text)</span><br><span class="line">words = [word <span class="keyword">for</span> word <span class="keyword">in</span> tokens <span class="keyword">if</span> word.isalpha()]</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Filter out Stop Words (and Pipeline)</strong></p>
<p>stop wordså¯¹è¯ç»„æ·±å±‚å«ä¹‰è¯æ²¡ä»€ä¹ˆå¸®åŠ©ï¼Œå®ƒä»¬é€šå¸¸æ˜¯<code>the</code>ã€<code>a</code>ã€<code>is</code>ç­‰ç­‰ã€‚NLTKæä¾›äº†ä¸€ç³»åˆ—ä¸åŒè¯­è¨€ç‰ˆæœ¬çš„åœç”¨è¯ï¼Œå®ƒä»¬å¯ä»¥è¢«è¿™æ ·è½½å…¥ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line">stop_words = stopwords.words(<span class="string">&#x27;english</span></span><br></pre></td></tr></table></figure>

<p>å®ƒä»¬æ‰“å°å‡ºæ¥éƒ½æ˜¯å°å†™ï¼Œæ‰€ä»¥åœ¨filter outå‰è®°å¾—æŠŠè¦å¤„ç†çš„æ–‡æœ¬ä¹Ÿå˜ä¸ºå°å†™ã€‚</p>
</li>
<li><p><strong>Stem Words</strong></p>
<p>Stemmingæ˜¯æŒ‡å°†æ¯ä¸ªè¯è¿˜åŸä¸ºè¯æ ¹æˆ–è¯åŸºçš„è¿‡ç¨‹ã€‚ä¸»æµçš„è€åŠæ³•æ˜¯Porter Stemming algorithmï¼Œå¯åœ¨nltkä¸­è½½å…¥PorterStemmer ç±»åä½¿ç”¨ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">porter=PorterStemmer()</span><br><span class="line">stemmed=[porter.stem(word) <span class="keyword">for</span> word <span class="keyword">in</span> tokens]</span><br></pre></td></tr></table></figure>



</li>
</ul>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li>Load the raw text. </li>
<li>Split into tokens. </li>
<li>Convert to lowercase. </li>
<li>Remove punctuation from each token. </li>
<li>Filter out remaining tokens that are not alphabetic. </li>
<li>Filter out tokens that are stop words</li>
</ul>
<h2 id="Additional-Text-Cleaning-Considerations"><a href="#Additional-Text-Cleaning-Considerations" class="headerlink" title="Additional Text Cleaning Considerations"></a>Additional Text Cleaning Considerations</h2><ul>
<li>Handling large documents and large collections of text documents that do not fit into memory. </li>
<li>Extracting text from markup like HTML, PDF, or other structured document formats. </li>
<li>Transliteration of characters from other languages into English. </li>
<li>Decoding Unicode characters into a normalized form, such as UTF8. </li>
<li>Handling of domain specific words, phrases, and acronyms. </li>
<li>Handling or removing numbers, such as dates and amounts. </li>
<li>Locating and correcting common typos and misspellings. </li>
<li>And much moreâ€¦</li>
</ul>
<h1 id="Bag-of-Words-Modelï¼ˆBowï¼‰"><a href="#Bag-of-Words-Modelï¼ˆBowï¼‰" class="headerlink" title="Bag-of-Words Modelï¼ˆBowï¼‰"></a>Bag-of-Words Modelï¼ˆBowï¼‰</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input:document Output: a class label</span><br><span class="line">convert documents to fixed-length vectors, the length is the length of the words,the value is a count or frequency of each word in encoded document.</span><br><span class="line">Each word should be encoded as a unique number</span><br></pre></td></tr></table></figure>

<p>ç¼–ç ä¸å…³å¿ƒé¡ºåºï¼Œåªå…³å¿ƒé¢‘ç‡ï¼Œæ‰€ä»¥æœ‰å¾ˆå¤šæ–¹æ³•å¯ä»¥è¿›è¡Œç¼–ç ï¼Œsklearnæä¾›äº†å¦‚ä¸‹ä¸‰ç§ç¼–ç æ–¹æ¡ˆã€‚</p>
<ul>
<li><p><strong>Word Counts with CountVectorizer</strong></p>
<ul>
<li><p>Create <font color=salmon>an instance of the CountVectorizer class</font>. </p>
</li>
<li><p>Call the<font color=salmon> fit() function </font>in order to learn a vocabulary from one or more documents. </p>
</li>
<li><p>Call the <font color=salmon>transform() function </font>on one or more documents as needed to encode each as a vector.</p>
</li>
</ul>
<blockquote>
<p>An encoded vector is returned with<font color=chocolate> a length of the entire vocabulary</font> and <font color=chocolate> an integer count for the number of times each word appeared in the document</font>. Because these vectors will contain a lot of zeros, we call them <font color=Crimson><strong>sparse</strong></font>.</p>
</blockquote>
<p>ä½¿ç”¨<code>scipy.sparce</code>å¯ä»¥å¤„ç†è¿™äº›ç³»æ•°å‘é‡ã€‚è°ƒç”¨transform()è¿”å›çš„å‘é‡å°†æ˜¯ç¨€ç–å‘é‡ï¼Œä½ å¯ä»¥é€šè¿‡è°ƒç”¨toarray()å‡½æ•°ï¼Œå°†å®ƒä»¬è½¬æ¢å›NumPyæ•°ç»„æ¥æŸ¥çœ‹ï¼Œå¹¶æ›´å¥½åœ°ç†è§£å‘ç”Ÿäº†ä»€ä¹ˆäº‹ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="comment"># list of text documents</span></span><br><span class="line">text = [<span class="string">&quot;The quick brown fox jumped over the lazy dog.&quot;</span>]</span><br><span class="line"><span class="comment"># create the transform</span></span><br><span class="line">vectorizer = CountVectorizer()</span><br><span class="line"><span class="comment"># tokenize and build vocab</span></span><br><span class="line">vectorizer.fit(text)</span><br><span class="line"><span class="comment"># summarize</span></span><br><span class="line">print(vectorizer.vocabulary_)</span><br><span class="line"><span class="comment"># encode document</span></span><br><span class="line">vector = vectorizer.transform(text)</span><br><span class="line"><span class="comment"># summarize encoded vector</span></span><br><span class="line">print(vector.shape)</span><br><span class="line">print(type(vector))</span><br><span class="line">print(vector.toarray())</span><br></pre></td></tr></table></figure>

<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ptint(vectorizer.vocabulary_)</span><br><span class="line">print(vector.shape)</span><br><span class="line">print(type(vector))</span><br><span class="line">print(vector.toarray())</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#39;dog&#39;: 1, &#39;fox&#39;: 2, &#39;over&#39;: 5, &#39;brown&#39;: 0, &#39;quick&#39;: 6, &#39;the&#39;: 7, &#39;lazy&#39;: 4, &#39;jumped&#39;: 3&#125;</span><br><span class="line">(1, 8)</span><br><span class="line">&lt;class &#39;scipy.sparse.csr.csr_matrix&#39;&gt;</span><br><span class="line">[[1 1 1 1 1 1 1 2]]</span><br></pre></td></tr></table></figure>

<p>æ‰€æœ‰å•è¯éƒ½æ˜¯é»˜è®¤å°å†™ï¼Œæ ‡ç‚¹ç¬¦å·å·²è¢«å»æ‰ï¼Œå‘é‡é•¿åº¦ä¸º8ï¼Œç¼–ç å‘é‡æ˜¯ç¨€ç–çŸ©é˜µã€‚</p>
<p>å¯ä»¥ç”¨åŒä¸€ä¸ªvectorizerå»è®¡ç®—ä¸åŒæ–‡ç« çš„sparse vectorã€‚æ¯”å¦‚ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">text2=[<span class="string">&quot;the puppy&quot;</span>]</span><br><span class="line">vector2=vectorizer.transform(text2)</span><br><span class="line">print(vector2.toarray())</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Word Frequencies with TfidfVectorizer</strong></p>
<p>æœ‰äº›è¯ï¼Œæ¯”å¦‚theï¼Œå‡ºç°æ¬¡æ•°å¤ªå¤šäº†ä»¥åŠå°±ä¸å¤§äº†ã€‚æ‰€ä»¥ï¼Œä¸€ç§æ›¿ä»£æ–¹æ¡ˆæ˜¯è®¡ç®—è¯é¢‘ï¼Œä¸€ä¸ªä¸»æµåŠæ³•æ˜¯ TF-IDFï¼ˆå³Term Frequency - Inverse Document Frequencyï¼‰</p>
<ul>
<li><strong>Term Frequency</strong>: This summarizes how often a given word appears within a document. </li>
<li><strong>Inverse Document Frequency</strong>: This downscales words that appear a lot across documents.ï¼ˆè¿™æ˜¯å¯¹æ–‡æ¡£ä¸­å‡ºç°é¢‘ç‡è¾ƒé«˜çš„è¯è¿›è¡Œé™é¢‘ï¼‰</li>
</ul>
<p>TF-IDFæ˜¯è¯é¢‘åˆ†æ•°ï¼Œå°½é‡çªå‡ºçš„æ˜¯æ¯”è¾ƒæœ‰æ„æ€çš„è¯ï¼Œæ¯”å¦‚åœ¨ä¸€ç¯‡æ–‡æ¡£ä¸­é¢‘ç¹å‡ºç°ï¼Œä½†åœ¨ä¸åŒæ–‡æ¡£ä¸­æ²¡æœ‰å‡ºç°ã€‚</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31197209">link</a></p>
<blockquote>
<p>ç¬¬ä¸€æ­¥ï¼Œè®¡ç®—è¯é¢‘</p>
<p><img src="https://cdn.jsdelivr.net/gh/AllMainAshley/CDN@master/images/20201116213953.png" loading="lazy"></p>
<p>è€ƒè™‘åˆ°æ–‡ç« æœ‰é•¿çŸ­ä¹‹åˆ†ï¼Œä¸ºäº†ä¾¿äºä¸åŒæ–‡ç« çš„æ¯”è¾ƒï¼Œè¿›è¡Œâ€è¯é¢‘â€æ ‡å‡†åŒ–.</p>
<p><img src="https://cdn.jsdelivr.net/gh/AllMainAshley/CDN@master/images/20201116214144.png" loading="lazy"></p>
<p>ç¬¬äºŒæ­¥ï¼Œè®¡ç®—é€†æ–‡æ¡£é¢‘ç‡ï¼š</p>
<p>è¿™æ—¶ï¼Œéœ€è¦ä¸€ä¸ªè¯­æ–™åº“ï¼ˆcorpusï¼‰ï¼Œç”¨æ¥æ¨¡æ‹Ÿè¯­è¨€çš„ä½¿ç”¨ç¯å¢ƒã€‚</p>
<p><img src="https://cdn.jsdelivr.net/gh/AllMainAshley/CDN@master/images/20201116214304.jpg" loading="lazy"></p>
<p>å¦‚æœä¸€ä¸ªè¯è¶Šå¸¸è§ï¼Œé‚£ä¹ˆåˆ†æ¯å°±è¶Šå¤§ï¼Œé€†æ–‡æ¡£é¢‘ç‡å°±è¶Šå°è¶Šæ¥è¿‘0ã€‚åˆ†æ¯ä¹‹æ‰€ä»¥è¦åŠ 1ï¼Œæ˜¯ä¸ºäº†é¿å…åˆ†æ¯ä¸º0ï¼ˆå³æ‰€æœ‰æ–‡æ¡£éƒ½ä¸åŒ…å«è¯¥è¯ï¼‰ã€‚logè¡¨ç¤ºå¯¹å¾—åˆ°çš„å€¼å–å¯¹æ•°ã€‚</p>
<p>ç¬¬ä¸‰æ­¥ï¼Œè®¡ç®—TF-IDFï¼š</p>
<p><img src="https://cdn.jsdelivr.net/gh/AllMainAshley/CDN@master/images/20201116214234.png" loading="lazy"></p>
<p>å¯ä»¥çœ‹åˆ°ï¼ŒTF-IDFä¸ä¸€ä¸ªè¯åœ¨æ–‡æ¡£ä¸­çš„å‡ºç°æ¬¡æ•°æˆæ­£æ¯”ï¼Œä¸è¯¥è¯åœ¨æ•´ä¸ªè¯­è¨€ä¸­çš„å‡ºç°æ¬¡æ•°æˆåæ¯”ã€‚æ‰€ä»¥ï¼Œè‡ªåŠ¨æå–å…³é”®è¯çš„ç®—æ³•å°±å¾ˆæ¸…æ¥šäº†ï¼Œå°±æ˜¯<strong>è®¡ç®—å‡ºæ–‡æ¡£çš„æ¯ä¸ªè¯çš„TF-IDFå€¼ï¼Œç„¶åæŒ‰é™åºæ’åˆ—ï¼Œå–æ’åœ¨æœ€å‰é¢çš„å‡ ä¸ªè¯ã€‚</strong></p>
<h2 id="ä¼˜ç¼ºç‚¹"><a href="#ä¼˜ç¼ºç‚¹" class="headerlink" title="ä¼˜ç¼ºç‚¹"></a><strong>ä¼˜ç¼ºç‚¹</strong></h2><p>TF-IDFçš„ä¼˜ç‚¹æ˜¯ç®€å•å¿«é€Ÿï¼Œè€Œä¸”å®¹æ˜“ç†è§£ã€‚ç¼ºç‚¹æ˜¯æœ‰æ—¶å€™ç”¨<strong>è¯é¢‘</strong>æ¥è¡¡é‡æ–‡ç« ä¸­çš„ä¸€ä¸ªè¯çš„é‡è¦æ€§ä¸å¤Ÿå…¨é¢ï¼Œæœ‰æ—¶å€™é‡è¦çš„è¯å‡ºç°çš„å¯èƒ½ä¸å¤Ÿå¤šï¼Œè€Œä¸”è¿™ç§è®¡ç®—æ— æ³•ä½“ç°ä½ç½®ä¿¡æ¯ï¼Œæ— æ³•ä½“ç°è¯åœ¨ä¸Šä¸‹æ–‡çš„é‡è¦æ€§ã€‚å¦‚æœè¦ä½“ç°è¯çš„ä¸Šä¸‹æ–‡ç»“æ„ï¼Œé‚£ä¹ˆä½ å¯èƒ½éœ€è¦ä½¿ç”¨word2vecç®—æ³•æ¥æ”¯æŒã€‚</p>
</blockquote>
</li>
<li><p><strong>HashingVectorizer</strong></p>
<p>vocabularyå¯èƒ½å¾ˆå¤§ï¼Œæ‰€ä»¥å¯ä»¥ä½¿ç”¨hashçš„æ–¹æ³•æŠŠä»–ä»¬è½¬æ¢ä¸ºè£…æŸï¼Œè¿™æ ·è¯æ±‡ä¹Ÿä¸æ˜¯å®šé•¿çš„äº†ï¼Œç¼ºç‚¹æ˜¯hashæ˜¯å•å‘çš„ï¼Œä¹Ÿå°±æ²¡æœ‰åŠæ³•æŠŠä»–ä»¬ä»ç¼–ç è½¬æ¢æˆè¯è¯­äº†ã€‚ä½œè€…æƒ³æƒ³æœ‰ä¸€äº›å¯å‘å¼çš„æ–¹æ³•ï¼Œä½ å¯ä»¥æ ¹æ®ä¼°è®¡çš„è¯æ±‡é‡å¤§å°æ¥æŒ‘é€‰å“ˆå¸Œé•¿åº¦å’Œç¢°æ’çš„æ¦‚ç‡ï¼ˆä¾‹å¦‚75%çš„è´Ÿè½½ç³»æ•°ï¼‰ã€‚</p>
<p>ç¼–ç æ–‡æ¡£çš„å€¼é»˜è®¤å¯¹åº”äº-1åˆ°1èŒƒå›´å†…çš„å½’ä¸€åŒ–å­—æ•°ï¼Œä½†å¯ä»¥é€šè¿‡æ”¹å˜é»˜è®¤é…ç½®ä½¿å…¶æˆä¸ºç®€å•çš„æ•´æ•°ã€‚</p>
</li>
</ul>
<h1 id="Prepare-Data-With-Keras"><a href="#Prepare-Data-With-Keras" class="headerlink" title="Prepare Data With Keras"></a>Prepare Data With Keras</h1><p> Keras provides the text to word sequence() function that you can use to split text into a list of words. By default, this function automatically does 3 things: </p>
<ul>
<li>Splits words by space. </li>
<li>Encoding with one hot </li>
<li>Filters out punctuation. </li>
<li>Converts text to lowercase (lower=True)</li>
</ul>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> text_to_word_sequence</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> one_hot</span><br><span class="line"></span><br><span class="line">text=<span class="string">&#x27;The quick brown fox jumped over the lazy dog.&#x27;</span></span><br><span class="line"><span class="comment"># words=text_to_word_sequence(text)</span></span><br><span class="line"><span class="comment"># vocab_size=len(words)</span></span><br><span class="line"><span class="comment"># print(vocab_size)</span></span><br><span class="line"></span><br><span class="line">words=set(text_to_word_sequence(text))  <span class="comment"># å»é™¤é‡å¤çš„å•è¯</span></span><br><span class="line"><span class="comment"># print(text_to_word_sequence(text))</span></span><br><span class="line"><span class="comment"># print(words)</span></span><br><span class="line">vocab_size=len(words)</span><br><span class="line">print(vocab_size)</span><br><span class="line">result=one_hot(text,round(vocab_size*<span class="number">1.3</span>))  <span class="comment"># ?</span></span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>

<hr>
<ul>
<li><strong>hashing_trick</strong></li>
</ul>
<p>A limitation of integer and count base encodings is that they must maintain a vocabulary of words and their mapping to integers.An alternative to this approach is to use a one-way hash function to convert words to integers.</p>
<p>Keras provides the hashing trick() function that tokenizes and then integer encodes the document, just like the one hot() function.</p>
<hr>
<ul>
<li><p><strong>Tokenizer</strong></p>
<p>Once fit, the Tokenizer provides 4 attributes that you can use to query what has been learned about your documents: </p>
<ul>
<li>word counts: A dictionary mapping of words and their occurrence counts when the Tokenizer was fit. </li>
<li>word docs: A dictionary mapping of words and the number of documents that reach appears in. </li>
<li>word index: A dictionary of words and their uniquely assigned integers.</li>
<li>document count: A dictionary mapping and the number of documents they appear in calculated during the fit.</li>
</ul>
</li>
</ul>
<hr>
<p>The texts to matrix() function on the Tokenizer can be used to create one vector per document provided per input. The length of the vectors is the total size of the vocabulary. </p>
<ul>
<li>binary: Whether or not each word is present in the document. This is the default. </li>
<li>count: The count of each word in the document. </li>
<li>tfidf: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document. </li>
<li>freq: The frequency of each word as a ratio of words within each document. We can put all of this together with a worked example</li>
</ul>
<hr>
<ul>
<li>A vocabulary of known words. </li>
<li>A measure of the presence of known words.</li>
</ul>
<h1 id="Bags-of-Words"><a href="#Bags-of-Words" class="headerlink" title="Bags of Words"></a>Bags of Words</h1><p><strong>Counter</strong>, which is a dictionary mapping of words and their count that allows us to easily update and query</p>
<hr>
<p>texts to matrix() </p>
<ul>
<li>binary è¢«æ ‡è®°ä¸º0æˆ–1</li>
<li>count æ¯ä¸ªå•è¯å‡ºç°æ¬¡æ•°</li>
<li>tfidf è¯é¢‘å’Œé€†è¯é¢‘</li>
<li>freq è¯é¢‘</li>
</ul>
<hr>
<p><strong>Word Embedding</strong></p>
<p>ç”¨å‘é‡ç©ºé—´è¡¨ç¤ºè¯è¯­ã€‚</p>
<hr>
<p>Word2Vecæ¨¡å‹ä¸­ï¼Œä¸»è¦æœ‰Skip-Gramå’ŒCBOWä¸¤ç§æ¨¡å‹ï¼Œä»ç›´è§‚ä¸Šç†è§£ï¼ŒSkip-Gramæ˜¯ç»™å®šinput wordæ¥é¢„æµ‹ä¸Šä¸‹æ–‡ã€‚è€ŒCBOWæ˜¯ç»™å®šä¸Šä¸‹æ–‡ï¼Œæ¥é¢„æµ‹input wordã€‚æœ¬ç¯‡æ–‡ç« ä»…è®²è§£Skip-Gramæ¨¡å‹ã€‚</p>
<p>Both models are focused on learning about words given their local usage context, where the<br>context is defined by a window of neighboring words. This window is a configurable parameter<br>of the model.<br>The size of the sliding window has a strong effect on the resulting vector similarities.<br>Large windows tend to produce more topical similarities [â€¦], while smaller windows<br>tend to produce more functional and syntactic similarities.</p>
<hr>
<p><strong>Embedding Layer</strong></p>
<p>input dim: This is the size of the vocabulary in the text data.</p>
<p>output dim: This is the size of the vector space in which words will be embedded.</p>
<p>input length: This is the length of input sequences.</p>
<p>The output of the Embedding layer is a 2D vector with one embedding for each word in the input sequence of words (input document). If you wish to connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output matrix to a 1D vector using the Flatten layer. Now, letâ€™s see how we can use an Embedding layer in practice.</p>
<hr>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/55412623">link: text_to sequences</a></p>
<p><code>texts_to_sequences</code>è¾“å‡ºçš„æ˜¯æ ¹æ®å¯¹åº”å…³ç³»è¾“å‡ºçš„å‘é‡åºåˆ—ï¼Œæ˜¯ä¸å®šé•¿çš„ï¼Œè·Ÿå¥å­çš„é•¿åº¦æœ‰å…³ç³»ã€‚</p>
<ul>
<li>word_countsï¼šè¯é¢‘ç»Ÿè®¡ç»“æœ</li>
<li>word_indexï¼šè¯å’Œindexçš„å¯¹åº”å…³ç³»</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line">text1=<span class="string">&#x27;Some ThING to eat !&#x27;</span></span><br><span class="line">text2=<span class="string">&#x27;some thing to drink .&#x27;</span></span><br><span class="line">texts=[text1,text2]</span><br><span class="line">print(texts)</span><br><span class="line"><span class="comment">#out:[&#x27;Some ThING to eat !&#x27;, &#x27;some thing to drink .&#x27;]</span></span><br><span class="line">tokenizer = Tokenizer(num_words=<span class="number">100</span>) <span class="comment">#num_words:Noneæˆ–æ•´æ•°,å¤„ç†çš„æœ€å¤§å•è¯æ•°é‡ã€‚å°‘äºæ­¤æ•°çš„å•è¯ä¸¢æ‰</span></span><br><span class="line">tokenizer.fit_on_texts(texts)</span><br><span class="line">print( tokenizer.word_counts) </span><br><span class="line"><span class="comment">#out:OrderedDict([(&#x27;some&#x27;, 2), (&#x27;thing&#x27;, 2), (&#x27;to&#x27;, 2), (&#x27;eat&#x27;, 1), (&#x27;drink&#x27;, 1)])</span></span><br><span class="line">print( tokenizer.word_index) </span><br><span class="line"><span class="comment">#out:&#123;&#x27;some&#x27;: 1, &#x27;thing&#x27;: 2, &#x27;to&#x27;: 3, &#x27;eat&#x27;: 4, &#x27;drink&#x27;: 5&#125;</span></span><br><span class="line">sequences = tokenizer.texts_to_sequences(texts)</span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line">print(sequences)</span><br><span class="line"><span class="comment">#out:[[1, 2, 3, 4], [1, 2, 3, 5]] è½¬æ¢ä¸ºåºåˆ—ï¼Œæ³¨æ„è¿™é‡Œå¥å­ç­‰é•¿ï¼Œæ‰€ä»¥è¾“å‡ºä¸€æ ·ï¼Œä½†æ˜¯ä¸ç­‰é•¿å¥å­è¾“å‡ºçš„é•¿åº¦æ˜¯ä¸ä¸€æ ·çš„</span></span><br><span class="line">print(<span class="string">&#x27;Found %s unique tokens.&#x27;</span> % len(word_index))</span><br><span class="line"><span class="comment">#out:Found 5 unique tokens.</span></span><br></pre></td></tr></table></figure>

<p><code>pad_sequences</code>,å¯¹ä¸Šé¢ç”Ÿæˆçš„ä¸å®šé•¿åºåˆ—è¿›è¡Œè¡¥å…¨ã€‚å¯ä»¥æ‰‹åŠ¨è®¾å®šæ¯ä¸ªå¥å­çš„æœ€å¤§é•¿åº¦å‚æ•°ï¼Œå¤§äºè¿™ä¸ªé•¿åº¦æˆªæ–­ï¼Œå°äºè¿™ä¸ªé•¿åº¦å¡«å……ã€‚<strong>æ³¨æ„</strong>ï¼šé»˜è®¤è¡¥å…¨å’Œæˆªæ–­éƒ½æ˜¯åœ¨å¥å­å‰é¢è¿›è¡Œå¡«å……å’Œæˆªæ–­ã€‚è¿™é‡Œæ˜¯ç”¨0è¿›è¡Œå¡«å……ï¼Œä¹Ÿå°±æ˜¯ç©ºæ ¼ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆä¸Šé¢åºåˆ—indexèµ·å§‹æ˜¯1çš„åŸå› ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#æ¥ä¸Šé¢çš„ä»£ç </span></span><br><span class="line">SEQ_LEN = <span class="number">10</span></span><br><span class="line">data = pad_sequences(sequences, maxlen=SEQ_LEN)</span><br><span class="line">print(data)</span><br><span class="line"><span class="comment">#out:[[0 0 0 0 0 0 1 2 3 4]</span></span><br><span class="line"><span class="comment"># [0 0 0 0 0 0 1 2 3 5]]</span></span><br></pre></td></tr></table></figure>



<p><code>texts_to matrix</code>ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#ï¼ˆä¹Ÿå¯ä»¥ç›´æ¥å¾—åˆ° one-hot äºŒè¿›åˆ¶è¡¨ç¤ºã€‚ï¼‰è¿™ä¸ªåˆ†è¯å™¨ä¹Ÿæ”¯æŒé™¤ one-hot ç¼–ç å¤–çš„å…¶ä»–å‘é‡åŒ–æ¨¡å¼</span></span><br><span class="line">one_hot_results = tokenizer.texts_to_matrix(samples, mode=<span class="string">&#x27;binary&#x27;</span>)</span><br></pre></td></tr></table></figure>

<hr>
<p><strong>Summary</strong></p>
<ul>
<li>Embeddingå±‚æœ‰ä¸€ç³»åˆ—æ•´æ•°åºåˆ—ï¼Œå¯ä»¥ä½¿ç”¨æ›´åŠ å¤æ‚çš„è¯è¢‹æ¨¡å‹ã€‚Kerasæä¾›äº†one hotå‡½æ•°ï¼Œå¯¹æ¯ä¸ªè¯åˆ›å»ºäº†hashå€¼ã€‚<code>Embedding(vocab_size, 8, input_length=max_length)</code> è¯¥Embeddingå±‚æœ‰ä¸€ä¸ªå¤§å°ä¸º50çš„vocabularyï¼Œå’Œä¸€ä¸ªå¤§å°ä¸º4çš„inputï¼Œè¾“å‡ºè¯å‘é‡çš„ç»´åº¦æ˜¯8ã€‚å¦‚æœè¦æŠŠEmbeddingå±‚æ”¾å…¥Denseé‡Œï¼Œåˆ™éœ€è¦æŠŠmax_lengthä¸ª8ç»´å‘é‡Flatten() å±•å¹³ï¼Œå†å­˜å…¥Denseå±‚ä¸­ã€‚</li>
</ul>
<blockquote>
<p><code>Embedding(input_dim, output_dim, embeddings_initializer=&#39;uniform&#39;, embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)</code></p>
<ul>
<li><strong>input_dim</strong>: int &gt; 0ã€‚è¯æ±‡è¡¨å¤§å°ï¼Œ å³ï¼Œæœ€å¤§æ•´æ•° index + 1ã€‚</li>
<li><strong>output_dim</strong>: int &gt;= 0ã€‚è¯å‘é‡çš„ç»´åº¦ã€‚</li>
<li><strong>embeddings_initializer</strong>: <code>embeddings</code> çŸ©é˜µçš„åˆå§‹åŒ–æ–¹æ³• (è¯¦è§ <a target="_blank" rel="noopener" href="https://keras.io/zh/initializers/">initializers</a>)ã€‚</li>
<li><strong>embeddings_regularizer</strong>: <code>embeddings</code> matrix çš„æ­£åˆ™åŒ–æ–¹æ³• (è¯¦è§ <a target="_blank" rel="noopener" href="https://keras.io/zh/regularizers/">regularizer</a>)ã€‚</li>
<li><strong>embeddings_constraint</strong>: <code>embeddings</code> matrix çš„çº¦æŸå‡½æ•° (è¯¦è§ <a target="_blank" rel="noopener" href="https://keras.io/zh/constraints/">constraints</a>)ã€‚</li>
<li><strong>mask_zero</strong>: æ˜¯å¦æŠŠ 0 çœ‹ä½œä¸ºä¸€ä¸ªåº”è¯¥è¢«é®è”½çš„ç‰¹æ®Šçš„ â€œpaddingâ€ å€¼ã€‚ è¿™å¯¹äºå¯å˜é•¿çš„ <a target="_blank" rel="noopener" href="https://keras.io/zh/layers/recurrent/">å¾ªç¯ç¥ç»ç½‘ç»œå±‚</a> ååˆ†æœ‰ç”¨ã€‚ å¦‚æœè®¾å®šä¸º <code>True</code>ï¼Œé‚£ä¹ˆæ¥ä¸‹æ¥çš„æ‰€æœ‰å±‚éƒ½å¿…é¡»æ”¯æŒ maskingï¼Œå¦åˆ™å°±ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚ å¦‚æœ mask_zero ä¸º <code>True</code>ï¼Œä½œä¸ºç»“æœï¼Œç´¢å¼• 0 å°±ä¸èƒ½è¢«ç”¨äºè¯æ±‡è¡¨ä¸­ ï¼ˆinput_dim åº”è¯¥ä¸ vocabulary + 1 å¤§å°ç›¸åŒï¼‰ã€‚</li>
<li><strong>input_length</strong>: è¾“å…¥åºåˆ—çš„é•¿åº¦ï¼Œå½“å®ƒæ˜¯å›ºå®šçš„æ—¶ã€‚ å¦‚æœä½ éœ€è¦è¿æ¥ <code>Flatten</code> å’Œ <code>Dense</code> å±‚ï¼Œåˆ™è¿™ä¸ªå‚æ•°æ˜¯å¿…é¡»çš„ ï¼ˆæ²¡æœ‰å®ƒï¼Œdense å±‚çš„è¾“å‡ºå°ºå¯¸å°±æ— æ³•è®¡ç®—ï¼‰ã€‚</li>
</ul>
</blockquote>
<p><code>tokenizer.texts_to_sequences()</code>ï¼šç¼–ç ä¸ºæ•´æ•°å‘é‡åºåˆ—</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Gobsd/article/details/56485177">numpyä¸­arrayå’Œasarrayçš„åŒºåˆ«</a></p>
<hr>
<p>æ— ç›‘ç£é¢„è®­ç»ƒword vectorsåœ¨NLPä¸­æ•ˆæœå¾ˆå¥½</p>
<p>å¯¹äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨CNNçš„é¢„è®­ç»ƒé™æ€word vectorsåšå¾—å¾ˆå¥½ã€‚</p>
<hr>
<p>A standard deep learning model for text classification and sentiment analysis uses a word embedding layer and one-dimensional convolutional neural network</p>
<p><strong>N-gramæ˜¯ç›´æ¥ç»Ÿè®¡ä¸åŒçš„Nä¸ªè¯ä¹‹é—´ç»„åˆåœ¨ä¸€èµ·çš„æ¦‚ç‡</strong></p>
<p><strong>CNNæ˜¯é€šè¿‡å­¦ä¹ å¾—åˆ°ä¸åŒè¯ç»„åˆçš„æ¯ä¸ªkernelçš„æƒé‡ï¼ŒåŠ æƒå’Œæ¥è¾¾åˆ°æŸç§åˆ†ç±»æˆ–è€…å…¶ä»–ç›®çš„</strong></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/yizhuanlu9607/article/details/78084266">â€˜râ€™ å’Œ â€˜rtâ€™ã€â€™wâ€™ å’Œ â€˜wtâ€™çš„åŒºåˆ«</a><br>rï¼šPython  å°†ä¼šæŒ‰ç…§ç¼–ç æ ¼å¼è¿›è¡Œè§£æï¼Œread() æ“ä½œè¿”å›çš„æ˜¯str</p>
<p>rbï¼šä¹Ÿå³ binary  modeï¼Œread()  æ“ä½œè¿”å›çš„æ˜¯bytes</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/xgh1951/article/details/80392411">pythonä¸­dump å’Œdumps loadå’Œloadsçš„åŒºåˆ«</a></p>
<p> é™¤äº†æ–‡æ¡£è¯´çš„è¯ï¼Œæ²¡æœ‰ä»€ä¹ˆå¯æ·»åŠ çš„ã€‚å¦‚æœè¦å°†JSONè½¬å‚¨åˆ°æ–‡ä»¶/å¥—æ¥å­—æˆ–å…¶ä»–æ–‡ä»¶ä¸­ï¼Œåˆ™åº”ä½¿ç”¨<code>dump()</code>ã€‚å¦‚æœåªéœ€è¦å®ƒä½œä¸ºå­—ç¬¦ä¸²ï¼ˆç”¨äºæ‰“å°ï¼Œè§£æç­‰ï¼‰ï¼Œåˆ™ä½¿ç”¨<code>dumps()</code>ï¼ˆè½¬å‚¨å­—ç¬¦ä¸²ï¼‰<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/36059194/what-is-the-difference-between-json-dump-and-json-dumps-in-python">link</a></p>
<hr>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_34474705/article/details/74458605">numpy.array</a></p>
<ul>
<li>Pythonä¸­æä¾›äº†listå®¹å™¨ï¼Œå¯ä»¥å½“ä½œæ•°ç»„ä½¿ç”¨ã€‚ä½†åˆ—è¡¨ä¸­çš„å…ƒç´ å¯ä»¥æ˜¯ä»»ä½•å¯¹è±¡ï¼Œå› æ­¤åˆ—è¡¨ä¸­ä¿å­˜çš„æ˜¯å¯¹è±¡çš„æŒ‡é’ˆï¼Œè¿™æ ·ä¸€æ¥ï¼Œä¸ºäº†ä¿å­˜ä¸€ä¸ªç®€å•çš„åˆ—è¡¨[1,2,3]ã€‚å°±éœ€è¦ä¸‰ä¸ªæŒ‡é’ˆå’Œä¸‰ä¸ªæ•´æ•°å¯¹è±¡ã€‚å¯¹äºæ•°å€¼è¿ç®—æ¥è¯´ï¼Œè¿™ç§ç»“æ„æ˜¾ç„¶ä¸å¤Ÿé«˜æ•ˆã€‚</li>
<li>Pythonè™½ç„¶ä¹Ÿæä¾›äº†arrayæ¨¡å—ï¼Œä½†å…¶åªæ”¯æŒä¸€ç»´æ•°ç»„ï¼Œä¸æ”¯æŒå¤šç»´æ•°ç»„ï¼Œä¹Ÿæ²¡æœ‰å„ç§è¿ç®—å‡½æ•°ã€‚å› è€Œä¸é€‚åˆæ•°å€¼è¿ç®—ã€‚</li>
<li>NumPyçš„å‡ºç°å¼¥è¡¥äº†è¿™äº›ä¸è¶³ã€‚</li>
</ul>
<hr>
<ul>
<li><p>ä»€ä¹ˆæ˜¯n-gramæ¨¡å‹ï¼Ÿ<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32829048">link</a></p>
<p>æ–‡æœ¬é‡Œé¢çš„å†…å®¹æŒ‰ç…§å­—èŠ‚è¿›è¡Œå¤§å°ä¸ºNçš„æ»‘åŠ¨çª—å£æ“ä½œï¼Œå½¢æˆäº†é•¿åº¦æ˜¯Nçš„å­—èŠ‚ç‰‡æ®µåºåˆ—ã€‚</p>
<blockquote>
<p> è¯¥æ¨¡å‹åŸºäºè¿™æ ·ä¸€ç§å‡è®¾ï¼Œç¬¬Nä¸ªè¯çš„å‡ºç°åªä¸å‰é¢N-1ä¸ªè¯ç›¸å…³ï¼Œè€Œä¸å…¶å®ƒä»»ä½•è¯éƒ½ä¸ç›¸å…³ï¼Œæ•´å¥çš„æ¦‚ç‡å°±æ˜¯å„ä¸ªè¯å‡ºç°æ¦‚ç‡çš„ä¹˜ç§¯ã€‚è¿™äº›æ¦‚ç‡å¯ä»¥é€šè¿‡ç›´æ¥ä»è¯­æ–™ä¸­ç»Ÿè®¡Nä¸ªè¯åŒæ—¶å‡ºç°çš„æ¬¡æ•°å¾—åˆ°ã€‚å¸¸ç”¨çš„æ˜¯äºŒå…ƒçš„Bi-Gramå’Œä¸‰å…ƒçš„Tri-Gramã€‚</p>
</blockquote>
<p>å¸¸è§åº”ç”¨ï¼šæœç´¢å¼•æ“ï¼ˆGoogleæˆ–è€…Baiduï¼‰ã€æˆ–è€…è¾“å…¥æ³•çš„çŒœæƒ³æˆ–è€…æç¤º</p>
</li>
</ul>
<p>texts_to_sequences</p>
<p>texts_to_matrix</p>
<hr>
<p>no formal specifications</p>
<p>An alternative approach to specifying the model of the language is to learn it from examples.</p>
<p>è¾ƒç®€å•çš„æ¨¡å‹å¯èƒ½ä¼šçœ‹ä¸€ä¸ªçŸ­è¯åºåˆ—çš„ä¸Šä¸‹æ–‡ï¼Œè€Œè¾ƒå¤§çš„æ¨¡å‹å¯èƒ½ä¼šåœ¨å¥å­æˆ–æ®µè½çš„å±‚æ¬¡ä¸Šå·¥ä½œã€‚æœ€å¸¸è§çš„æ˜¯ï¼Œè¯­è¨€æ¨¡å‹åœ¨è¯çš„æ°´å¹³ã€‚</p>
<p>Language modeling is the art of determining the probability of a sequence of words</p>
<p>è¯­è¨€æ¨¡å‹å¾ˆé‡è¦ã€‚</p>
<p>Neural Language Model æ¯”ç»å…¸æ–¹æ³•æ›´å¥½ when models are incorporated into larger models on challenging tasks like speech recognition and machine translationã€‚key reasonï¼šthe methodâ€™s ability to generalize.</p>
<p>é€šè¿‡ä»¥ä¸‹æ–¹å¼è§£å†³n-gramæ•°æ®ç¨€ç–æ€§é—®é¢˜ å°†å•è¯å‚æ•°åŒ–ä¸ºå‘é‡ï¼ˆå•è¯åµŒå…¥ï¼‰ï¼Œå¹¶å°†å…¶ä½œä¸ºè¾“å…¥åˆ° ä¸€ä¸ªç¥ç»ç½‘ç»œã€‚</p>
<p>n-gramï¼Ÿ</p>
<p>distributed representation approach</p>
<ol>
<li>å°†è¯æ±‡ä¸­çš„æ¯ä¸ªå•è¯ä¸åˆ†å¸ƒå¼å•è¯ç‰¹å¾å‘é‡å…³è”èµ·æ¥ã€‚</li>
<li>ç”¨ç‰¹å¾å‘é‡æ¥è¡¨ç¤ºè¯åºåˆ—çš„è”åˆæ¦‚ç‡å‡½æ•°ï¼Œå…¶ç‰¹å¾å‘é‡ä¸ºè¿™äº›è¯çš„é¡ºåºã€‚</li>
<li>åŒæ—¶å­¦ä¹ å•è¯ç‰¹å¾å‘é‡å’Œæ¦‚ç‡å‡½æ•°ã€‚</li>
</ol>
</div><div id="reward-container"><span class="hty-icon-button button-glow" id="reward-button" title="Donate" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === &quot;none&quot;) ? &quot;block&quot; : &quot;none&quot;;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span><div id="reward-comment">I'm so cute. Please give me money.</div><div id="qr" style="display:none;"><div style="display:inline-block"><a target="_blank" rel="noopener" href="https://cdn.jsdelivr.net/gh/AllMainAshley/CDN@master/images/20210119090449.jpg"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/AllMainAshley/CDN@master/images/20210119090449.jpg" alt="æ”¯ä»˜å®" title="æ”¯ä»˜å®"></a><div><span style="color:#00A3EE"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-alipay-line"></use></svg></span></div></div><div style="display:inline-block"><a target="_blank" rel="noopener" href="https://cdn.jsdelivr.net/gh/AllMainAshley/CDN@master/images/20210119090448.png"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/AllMainAshley/CDN@master/images/20210119090448.png" alt="QQ æ”¯ä»˜" title="QQ æ”¯ä»˜"></a><div><span style="color:#12B7F5"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-qq-line"></use></svg></span></div></div><div style="display:inline-block"><a target="_blank" rel="noopener" href="https://cdn.jsdelivr.net/gh/AllMainAshley/CDN@master/images/20210119090450.png"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/AllMainAshley/CDN@master/images/20210119090450.png" alt="å¾®ä¿¡æ”¯ä»˜" title="å¾®ä¿¡æ”¯ä»˜"></a><div><span style="color:#2DC100"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wechat-pay-line"></use></svg></span></div></div></div></div><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>ç¨‹æµ·ç›</li><li class="post-copyright-link"><strong>Post link: </strong><a href="https://allmainashley.github.io/2020/11/16/Notes/Machine%20Learning/Deep%20Learning%20in%20NLP/" title="Deep Learning in NLP">https://allmainashley.github.io/2020/11/16/Notes/Machine%20Learning/Deep%20Learning%20in%20NLP/</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> unless otherwise stated.</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2020/12/26/Notes/OpenCV/OpenCV%20Learning%20Notes/" rel="prev" title="OpenCV Learning Notes"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">OpenCV Learning Notes</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2020/10/22/Notes/Machine%20Learning/Genetic%20Algorithms/" rel="next" title="Genetic Algorithms"><span class="post-nav-text">Genetic Algorithms</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>è‹¥æ‚¨æ—  GitHub è´¦å·ï¼Œå¯ç›´æ¥åœ¨ä¸‹æ–¹åŒ¿åè¯„è®ºã€‚</span><br><span>è‹¥æ‚¨æƒ³åŠæ—¶å¾—åˆ°å›å¤æé†’ï¼Œå»ºè®®è·³è½¬ GitHub Issues è¯„è®ºã€‚</span><br><span>è‹¥æ²¡æœ‰æœ¬æ–‡ Issueï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ Comment æ¨¡ç‰ˆæ–°å»ºã€‚</span><br><a class="hty-button hty-button--raised" id="github-issues" target="_blank" rel="noopener" href="https://github.com/AllMainAshley/AllMainAshley.github.io/issues?q=is:issue+Deep Learning in NLP">GitHub Issues</a></div><div id="valine-container"></div><script>Yun.utils.getScript("https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js", () => {
  const valineConfig = {"enable":true,"appId":"duRJqtj9W4MnlhHrkhx0O3vb-gzGzoHsz","appKey":"7GxBY57ustzpxvpwNr39XneW","placeholder":"I want to say...","avatar":null,"pageSize":10,"visitor":false,"highlight":true,"recordIP":false,"enableQQ":true,"el":"#valine-container","lang":"en"}
  valineConfig.path = "/2020/11/16/Notes/Machine%20Learning/Deep%20Learning%20in%20NLP/"
  new Valine(valineConfig)
}, window.Valine);</script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2020 â€“ 2021 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> ç¨‹æµ·ç›</span></div><div class="powered"><span>Powered by <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> v5.2.0</span><span class="footer-separator">|</span><span>Theme - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.6.1</span></div><div class="live_time"><span>æœ¬åšå®¢å·²èŒèŒå“’åœ°è¿è¡Œ</span><span id="display_live_time"></span><span class="moe-text">(â—'â—¡'â—)</span><script>function blog_live_time() {
  setTimeout(blog_live_time, 1000);
  const start = new Date('2020-05-10T00:00:00');
  const now = new Date();
  const timeDiff = (now.getTime() - start.getTime());
  const msPerMinute = 60 * 1000;
  const msPerHour = 60 * msPerMinute;
  const msPerDay = 24 * msPerHour;
  const passDay = Math.floor(timeDiff / msPerDay);
  const passHour = Math.floor((timeDiff % msPerDay) / 60 / 60 / 1000);
  const passMinute = Math.floor((timeDiff % msPerHour) / 60 / 1000);
  const passSecond = Math.floor((timeDiff % msPerMinute) / 1000);
  display_live_time.innerHTML = " " + passDay + " å¤© " + passHour + " å°æ—¶ " + passMinute + " åˆ† " + passSecond + " ç§’";
}
blog_live_time();
</script></div></footer><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a><a class="popup-trigger hty-icon-button icon-search" id="search" href="javascript:;" title="Search"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-search-line"></use></svg></span></a><script>window.addEventListener("DOMContentLoaded", () => {
  // Handle and trigger popup window
  document.querySelector(".popup-trigger").addEventListener("click", () => {
    document.querySelector(".popup").classList.add("show");
    setTimeout(() => {
      document.querySelector(".search-input").focus();
    }, 100);
  });

  // Monitor main search box
  const onPopupClose = () => {
    document.querySelector(".popup").classList.remove("show");
  };

  document.querySelector(".popup-btn-close").addEventListener("click", () => {
    onPopupClose();
  });

  window.addEventListener("keyup", event => {
    if (event.key === "Escape") {
      onPopupClose();
    }
  });
});
</script><script src="/js/search/local-search.js" defer></script><div class="popup search-popup"><div class="search-header"><span class="popup-btn-close close-icon hty-icon-button"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-close-line"></use></svg></span></div><div class="search-input-container"><input class="search-input" id="local-search-input" type="text" placeholder="Searching..." value=""></div><div id="local-search-result"></div></div></div></body></html>